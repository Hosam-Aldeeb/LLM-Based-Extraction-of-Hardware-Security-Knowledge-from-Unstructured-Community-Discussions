# Hardware Security Intelligence Pipeline - Docker Compose
# 
# Usage:
#   1. Copy .env.example to .env and add your API keys
#   2. Run: docker-compose up -d
#   3. Run pipeline: docker exec mcp-pipeline node process-all-remaining.js
#
# Services:
#   - ollama: Local LLM for embeddings (nomic-embed-text)
#   - pipeline: Node.js environment with all scripts

services:
  # Ollama service for local embeddings
  ollama:
    image: ollama/ollama:latest
    container_name: mcp-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    # Uncomment below for GPU support (Linux + nvidia-docker only)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Main pipeline container
  pipeline:
    build: .
    container_name: mcp-pipeline
    depends_on:
      - ollama
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      # Mount these folders so data persists outside container
      - ./discord-exports:/app/discord-exports
      - ./results:/app/results
      - ./threaded-conversations:/app/threaded-conversations
    # Keep container running for exec commands
    stdin_open: true
    tty: true
    command: tail -f /dev/null

volumes:
  ollama-data:



